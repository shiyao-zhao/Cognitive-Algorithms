{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Algorithms - Assignment 3\n",
    "Cognitive Algorithms        \n",
    "Summer Term 2020      \n",
    "Technische Universit√§t Berlin     \n",
    "Fachgebiet Maschinelles Lernen \n",
    "                    \n",
    "**Respective quiz due on June 3, 2020 23:59 via ISIS**\n",
    "\n",
    "**Answer the questions on Isis in 'Assignment 3 - Quiz' and copy code from this notebook where necessary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming\n",
    "---\n",
    "*Please keep in mind, that we will deduct one point for each unnecessary loop.*            \n",
    "Note that part 2 of this assignment consists of two tasks.\n",
    "\n",
    "### Task 1: Ordinary Least Squares\n",
    "In this assignment you will implement a linear regression and predict two dimensional hand positions from electromyographic (EMG) recordings obtained with high-density electrode arrays on the lower arm.  Download the data set ```myo_data.mat``` from the ISIS web site, if not done yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_myo_data(fname):\n",
    "    ''' Loads EMG data from <fname>                      \n",
    "    '''\n",
    "    # load the data\n",
    "    data = loadmat(fname)\n",
    "    # extract data and hand positions\n",
    "    X = data['training_data']\n",
    "    X = np.log(X)\n",
    "    Y = data['training_labels']\n",
    "    #Split data into training and test data\n",
    "    X_train = X[:, :5000]\n",
    "    X_test = X[:, 5000:]\n",
    "    Y_train = Y[:, :5000]\n",
    "    Y_test = Y[:, 5000:]\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ols(X_train, Y_train, llambda = 0):\n",
    "    ''' Trains ordinary least squares (ols) regression\n",
    "    Input:\n",
    "    X_train - DxN array of N data points with D features\n",
    "    Y\n",
    "    - D2xN array of length N with D2 multiple labels\n",
    "    llabmda - Regularization parameter\n",
    "    Output:\n",
    "    W\n",
    "    - DxD2 array, linear mapping used to estimate labels\n",
    "    with sp.dot(W.T, X)\n",
    "    '''\n",
    "    # your code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ols(W, X_test):\n",
    "    ''' Applys ordinary least squares (ols) regression\n",
    "        Input:\n",
    "            X_test - DxN array of N data points with D features\n",
    "            W      - DxD2 array, linear mapping used to estimate labels trained with train_ols\n",
    "        Output:\n",
    "            Y_test - D2xN array\n",
    "    '''\n",
    "    # your code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_handposition():\n",
    "    X_train, Y_train, X_test, Y_test = load_myo_data('myo_data.mat')\n",
    "    # compute weight vector with linear regression\n",
    "    W = train_ols(X_train, Y_train)\n",
    "    # predict hand positions\n",
    "    Y_hat_train = apply_ols(W, X_train)\n",
    "    Y_hat_test = apply_ols(W, X_test) \n",
    "        \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(Y_train[0,:1000], Y_train[1,:1000],'.k', label = 'true')\n",
    "    plt.plot(Y_hat_train[0,:1000], Y_hat_train[1,:1000],'.r', label = 'predicted')\n",
    "    plt.title('Training Data')\n",
    "    plt.xlabel('x position')\n",
    "    plt.ylabel('y position')\n",
    "    plt.legend(loc = 0)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(Y_test[0,:1000], Y_test[1,:1000], '.k')\n",
    "    plt.plot(Y_hat_test[0,:1000], Y_hat_test[1,:1000], '.r')\n",
    "    plt.title('Test Data')\n",
    "    plt.xlabel('x position')\n",
    "    plt.ylabel('y position')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(Y_train[1,:600], 'k', label = 'true')\n",
    "    plt.plot(Y_hat_train[1,:600], 'r--', label = 'predicted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('y position')\n",
    "    plt.legend(loc = 0)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(Y_test[1,:600], 'k')\n",
    "    plt.plot(Y_hat_test[1,:600], 'r--')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('y position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_assignment4():\n",
    "    ##Example without noise\n",
    "    x_train = np.array([[ 0,  0,  1 , 1], [ 0,  1,  0, 1]])\n",
    "    y_train = np.array([[0, 1, 1, 2]])\n",
    "    \n",
    "    w_est = train_ols(x_train, y_train) \n",
    "    w_est_ridge = train_ols(x_train, y_train, llambda=1)\n",
    "    \n",
    "    assert(np.all(w_est.T == [[1, 1]])) \n",
    "    assert(np.all(w_est_ridge.T == [[.75, .75]]))\n",
    "    \n",
    "    y_est = apply_ols(w_est, x_train)\n",
    "    assert(np.all(y_train == y_est)) \n",
    "    \n",
    "    print('No-noise-case tests passed')\n",
    "    \n",
    "    ##Example with noise\n",
    "    #Data generation\n",
    "    w_true = 4\n",
    "    X_train = np.arange(10)\n",
    "    X_train = X_train[None,:]\n",
    "    Y_train = w_true * X_train + np.random.normal(0,2,X_train.shape)\n",
    "    \n",
    "    #Regression \n",
    "    w_est = train_ols(X_train, Y_train) \n",
    "    Y_est = apply_ols(w_est, X_train)\n",
    "    \n",
    "    #Plot result\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(X_train.T, Y_train.T, '+', label='Train Data')\n",
    "    plt.plot(X_train.T, Y_est.T, label='Estimated regression')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (5 points)** Implement ordinary least squares regression (OLS) with an optional ridge parameter by completing the function stubs  ```train_ols``` and  ```apply_ols```. In ```train_ols```, you estimate a linear mapping $W$,    \n",
    "$$W = (X_{\\text{train}}X_{\\text{train}}^{\\top} + \\lambda I)^{-1}X_{\\text{train}}Y_{\\text{train}}^{\\top}$$       \n",
    "that optimally predicts the training labels from the training data, $X_{\\text{train}} \\in \\mathbb{R}^{D_X \\times N_{tr}}$,  $Y_{\\text{train}} \\in\\mathbb{R}^{D_Y \\times N_{tr}}$. Here, $\\lambda \\in \\mathbb R$ is the (optional) Ridge regularization parameter.  \n",
    "The function ```apply_ols``` than uses the weight vector to predict the (unknown) hand positions of new test data $X_{\\text{test}} \\in\\mathbb{R}^{D_X \\times N_{te}}$     \n",
    "$$Y_{\\text{test}} = W^{\\top}X_{\\text{test}}$$         \n",
    "The function  ```test_assignment4``` helps you to debug your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_assignment4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (1 point)**  The data set ```myo_data.mat``` consists of preprocessed EMG data $X$ and 2-dimensional stimulus labels $Y$. Labels are x/y positions of the hand during different hand movements. The function  ```load_myo_data```  loads the data and splits it into train and test data. Familiarize yourself with the data by answering the following questions:         \n",
    "How many time points $N_{tr}$ does the train set contain? How many time points $N_{te}$ does the test set contain? At each time point, at how many electrodes $D_X$ was the EMG collected? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_{tr} = $\n",
    "\n",
    "$N_{te} = $ \n",
    "\n",
    "$D_{X} = $                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can predict two dimensional hand positions by calling the function ```predict_handpositions```. It plots, for the train and the test data, the true hand position versus the estimated hand position. Familiarize yourself with the code and the meaning of the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_handposition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) (3 points)** Below true and predicted hand positions are visualized as plotted with the function ```predict_handposition()``` from the notebook when ```train_ols()``` and ```apply_ols()``` are correctly implemented. The first image shows results when the input data (muscle activations) are logarithmized  with X = sp.log(X) before application of the regression.\n",
    "![Figure_1](handpositionsLog.png)\n",
    "For the second image, the input data has not been logarithmized before applying the regression algorithm.\n",
    "![Figure_2](handpositionsNonlog.png)\n",
    "\n",
    "Based on this images, please complete the following questions by selecting the missing words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To judge the accuracy of a solution usually the performance on [training/test] data is analyzed. In the present case, better performance is achieved when the [non-logarithmized/logarithmized] input data is used.\n",
    "The performance on the training data is [worse/better/equal] compared to the performance on the test data. When the original data shows a logarithmic behavior, applying a log transformation can be used to [smoothen/filter/linearize] the data.\n",
    "If we cannot predict the labels ùëå perfectly by a linear regression on ùëã, the relationship between ùëã and ùëå [cannot be linear/must be linear/must be non-linear/can be linear or non-linear/cannot be non-linear]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Polynomial Regression (12 points)\n",
    "In Task 1 you implemented linear regression. You will see in this task, that aboves code can be used to obtain polynomial fits for data prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (2 points)**  To do so, first write a function  ``` create_trainmatrix``` which takes a data matrix containing the input vector $X$ and a polynomial degree M as inputs and returns a data matrix $Y$ that holds the polynomials of $X$. \n",
    "\n",
    "For example, for an input vector $X$ = [[ 0. 1. 2.]] and M = 3, the output should be:\n",
    "            $Y$ = [[   1.    1.    1. ]\n",
    "                 [   0.    1.    2. ]\n",
    "                 [   0.    1.    4. ]\n",
    "                 [   0.    1.    8. ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainmatrix(X, M):\n",
    "    '''\n",
    "    input:  X: matrix containing the input vector\n",
    "            M: degree of polynomial\n",
    "    output: Y: matrix containing the 0th to Mth polynomial of X, eg for X = [[ 0. 1. 2.]] and M = 3:\n",
    "            Y = [[   1.    1.    1. ]\n",
    "                 [   0.    1.    2. ]\n",
    "                 [   0.    1.    4. ]\n",
    "                 [   0.    1.    8. ]]\n",
    "    '''\n",
    "    # your code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (4 points)** Next, write a function run_ridge_regression, that first creates toy data from a sine function as follows:           \n",
    "$$x_i \\in \\{0, 1, 2, \\ldots, 10\\}, y_i = 3\\sin(x_i) + \\epsilon_i, \\; \\; \\epsilon_i \\sim \\mathcal{N}(0, 3)$$        \n",
    "where $\\mathcal{N}$(mean, standard deviation) denotes the Gaussian distribution and $i \\in \\{1, 2, \\ldots, 11\\}$ is an index. Then call your already created functions ```create_trainmatrix()```, ```train_ols()``` and ```apply_ols()``` to implement polynomial regression, which models the relationship between $y$ and $x$ as an $m$th order polynomial, i.e. \n",
    "$\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\ldots + w_m x^m$. The parameters $w_0, w_1, \\ldots , w_m \\in \\mathbb R$ are estimated by Ridge Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ridge_regression(m,llambda): \n",
    "    ''' input:  m: polynomial degree\n",
    "                llambda: ridge regression parameter\n",
    "        output: X_train, Y_train: x and y values of the noisy sine function\n",
    "                X_axis: x values for the regression curve, exactly ten times higher resolution than X_train\n",
    "                Phi_train: mapping of input data points (X_train) to matrix with polynomials of the datapoint (as created with create_trainmatrix)\n",
    "                w_est: linear mapping used to estimate labels\n",
    "                Y_est: array of estimations\n",
    "    '''\n",
    "    # your code here\n",
    "    return None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polynomial_regression(m,llambda):\n",
    "    X_train, Y_train, X_axis, Phi_train, w_est, Y_est = run_ridge_regression(m, llambda)\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(X_train.T, Y_train.T, '+k', label = 'Train Data', markersize=16)\n",
    "    #plot fit for m= 1 and llambda = 0\n",
    "    plt.plot(X_axis.T, Y_est.T, '-.g', linewidth = 2, label = 'estimation')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim([-6, 6])\n",
    "    plt.title(f\"Polynomial ridge regression for m = {m} and $\\lambda$ = {llambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the provided function ```plot_polynomial_regression()``` for various polynomial degrees and ridge parameters to familiarize yourself with their influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_polynomial_regression(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) (6 points)** \n",
    "\n",
    "![Figure_3](ridge_poly_regression.png)\n",
    "On the left hand side of the image above, regression curves for different polynomial degrees m = 1,4,10 are shown and no ridge regression is applied ($\\lambda$ = 0). \n",
    "On the right hand side of the image above, regression curves for different ridge parameters $\\lambda$ = 0,1,10000 are shown for a polynomial of degree m = 10.\n",
    "Using these plots select the right words to complete the sentences below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data on the right hand side of the image above is best predicted by the fit with degree [1/4/10]. The underlying trend in the data is best predicted by the fit with degree [1/4/10]. Unknown data should therefore be predicted using the fit with degree [1/4/10].\n",
    "\n",
    "For the largest value of $\\lambda$ on the right hand side of the image, the approximation is [a good fit/ overfitting/underfitting]. The best predictions are given for $\\lambda$ = [0/1/10000].\n",
    "\n",
    "For different data sets, the best parameters for m and $\\lambda$ for the case presented in the image will [generally result in good predictions/only result in good predictions for similar data/generally result in bad predictions/only result in bad predictions for similar data]. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
